{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4583106c-f814-4c32-8c0c-c45e5d73d8d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade threadpoolctl torch\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40354a19-3a47-48d0-9ef9-0e5477fd4584",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"test_table_raw\", \"dp_ml_titanic_test_raw\")\n",
    "test_table_raw = dbutils.widgets.get(\"test_table_raw\")\n",
    "pdf = spark.sql(f\"SELECT * FROM dp_ml_raw.dp_ml_titanic.{test_table_raw}\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd34fbb2-ec13-4751-9605-06d4bcc6451b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e556d3d6-b61f-48f6-9011-1f7a2b4c5117",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_pdf = pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f9a65c2-9729-4a30-bc31-7de7e054cfc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_pdf['Survived'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3929df2-adfc-40ef-b715-b2e418a2edaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "spark.sql(\"USE CATALOG `dp_ml_raw`\")\n",
    "spark.sql(\"USE SCHEMA `dp_ml_titanic`\")\n",
    "\n",
    "\n",
    "NAME = \"dp_ml_raw.dp_ml_titanic.titanic_feature_pipeline\"\n",
    "client = MlflowClient()\n",
    "\n",
    "latest_ver = max(int(mv.version) for mv in client.search_model_versions(f\"name='{NAME}'\"))\n",
    "print(latest_ver)\n",
    "uri = f\"models:/{NAME}/{latest_ver}\"\n",
    "from mlflow import spark as mlflow_spark\n",
    "pipeline = mlflow.sklearn.load_model(uri) \n",
    "# sdf = spark.createDataFrame(test_pdf)                 # requires an active SparkSession\n",
    "X_test_features = pipeline.transform(test_pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae0b1a9e-6aa7-45d4-b321-2914ea07c41a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(X_test_features, X_test_features.columns)\n",
    "X_test_features = X_test_features.drop(\"num__Survived\", axis=1, errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b9afc91-7c01-4309-937a-529d36b54028",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# make standard scaler\n",
    "\n",
    "\n",
    "NAME = \"dp_ml_raw.dp_ml_titanic.titanic-scaler\"\n",
    "client = MlflowClient()\n",
    "\n",
    "latest_ver = max(int(mv.version) for mv in client.search_model_versions(f\"name='{NAME}'\"))\n",
    "print(latest_ver)\n",
    "\n",
    "uri = f\"models:/{NAME}/{latest_ver}\"\n",
    "from mlflow import spark as mlflow_spark\n",
    "pipeline = mlflow.sklearn.load_model(uri) \n",
    "# sdf = spark.createDataFrame(test_pdf)                 # requires an active SparkSession\n",
    "X_test = pipeline.transform(X_test_features)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "086c1593-c95c-4b43-b2c0-74a788abde00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "NAME = \"dp_ml_raw.dp_ml_titanic.titanic-pytorch\"\n",
    "client = MlflowClient()\n",
    "\n",
    "# Use the champion alias\n",
    "uri = f\"models:/{NAME}@champion\"\n",
    "\n",
    "# Load the PyTorch model\n",
    "model = mlflow.pytorch.load_model(uri)\n",
    "model.eval()  # set to evaluation mode\n",
    "\n",
    "# Convert your test features into a Torch tensor\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "# Run inference (disable gradient tracking)\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor)\n",
    "\n",
    "print(y_pred[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "879cb3b2-71fb-4d4a-9b51-163994bdfe0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f12e95d-e377-44b6-a5d2-293c92e618b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "print(type(y_pred))\n",
    "y_np = y_pred.detach().cpu().numpy()\n",
    "print(y_np.shape)\n",
    "df = pd.DataFrame({\"prediction\": y_np.ravel()})\n",
    "print(df)\n",
    "# Add a primary-key-like column from the index\n",
    "df = df.reset_index().rename(columns={\"index\": \"row_id\"})\n",
    "df[\"row_id\"] = df[\"row_id\"].astype(\"int64\")  # ensure BIGINT-compatible\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb2fc9cd-b791-4fa0-a9f4-0e5c9dd03af6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import LongType\n",
    "#spark = SparkSession.builder.getOrCreate()\n",
    "spark.sql('USE CATALOG dp_ml_raw')\n",
    "spark.sql('CREATE SCHEMA IF NOT EXISTS dp_ml_raw.inference')\n",
    "sdf = spark.createDataFrame(df)\n",
    "# Make sure row_id is LongType in Spark as well\n",
    "sdf = sdf.withColumn(\"row_id\", sdf[\"row_id\"].cast(LongType()))\n",
    "\n",
    "full_table_name = \"dp_ml_raw.inference.titanic_pytorch_predictions\"\n",
    "\n",
    "# Overwrite the table with the latest predictions; change to \"append\" if you prefer accumulating\n",
    "(sdf.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(full_table_name))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "inference",
   "widgets": {
    "test_table_raw": {
     "currentValue": "dp_ml_titanic_test_raw",
     "nuid": "04ac688e-fc47-4b88-968b-487b0f0bf7d1",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dp_ml_titanic_test_raw",
      "label": null,
      "name": "test_table_raw",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "dp_ml_titanic_test_raw",
      "label": null,
      "name": "test_table_raw",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
