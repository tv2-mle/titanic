{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fedf752-7402-4fb3-8ce6-4ccd002c5583",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# In a new cell\n",
    "%pip install -U threadpoolctl torch shap\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "330351c2-c28f-4980-b5e6-98ed3c926fa2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text('features_table', \"titanic_features\")\n",
    "features_table = dbutils.widgets.get('features_table')\n",
    "pdf = spark.sql(f\"SELECT * FROM dp_ml_raw.features.{features_table}\").toPandas()\n",
    "cols = pdf.columns\n",
    "print(cols, len(cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3ebc45f-0188-4a28-a9ac-8ee7c3a66657",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Load features (includes target) ---\n",
    "# TABLE = \"dp_ml_raw.features.titanic_features\"\n",
    "# pdf = spark.table(TABLE).toPandas()\n",
    "\n",
    "y = pdf['num__Survived'].astype(int)                           # ensure int/binary target\n",
    "\n",
    "# Keep only numeric features; drop target and any obvious keys\n",
    "drop_cols = [\"rowId\", \"num__Survived\"]\n",
    "X = pdf.drop(columns=drop_cols)\n",
    "# X = X.select_dtypes(include=[\"number\"]).copy()        # features must be numeric for sklearn\n",
    "\n",
    "print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "\n",
    "# --- 3-fold CV with train vs validation log-loss (cross-entropy) ---\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "folds = 10\n",
    "skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "\n",
    "fold = 0\n",
    "for train_idx, val_idx in skf.split(X, y):\n",
    "    fold += 1\n",
    "    X_tr, X_va = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_tr, y_va = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    sc = StandardScaler(); X_tr = sc.fit_transform(X_tr).astype(\"float32\"); X_va = sc.transform(X_va).astype(\"float32\")\n",
    "\n",
    "    model = LogisticRegression(solver=\"lbfgs\", max_iter=1000)\n",
    "    model.fit(X_tr, y_tr)\n",
    "\n",
    "    # Predict probabilities for log-loss\n",
    "    p_tr = model.predict_proba(X_tr)[:, 1]\n",
    "    p_va = model.predict_proba(X_va)[:, 1]\n",
    "\n",
    "    tr_loss = log_loss(y_tr, p_tr)\n",
    "    va_loss = log_loss(y_va, p_va)\n",
    "    train_losses.append(tr_loss)\n",
    "    val_losses.append(va_loss)\n",
    "\n",
    "    yhat_tr = model.predict(X_tr)\n",
    "    yhat_va = model.predict(X_va)\n",
    "    tr_acc = accuracy_score(y_tr, yhat_tr)\n",
    "    va_acc = accuracy_score(y_va, yhat_va)\n",
    "    train_accs.append(tr_acc)\n",
    "    val_accs.append(va_acc)\n",
    "\n",
    "    print(f\"Fold {fold}: \"\n",
    "          f\"train log-loss={tr_loss:.4f}, val log-loss={va_loss:.4f} | \"\n",
    "          f\"train acc={tr_acc:.4f}, val acc={va_acc:.4f}\")\n",
    "\n",
    "print(f\"\\n=== {folds}-fold summary ===\")\n",
    "print(f\"Train log-loss: mean={np.mean(train_losses):.4f} ± {np.std(train_losses):.4f}\")\n",
    "print(f\"Valid log-loss: mean={np.mean(val_losses):.4f} ± {np.std(val_losses):.4f}\")\n",
    "print(f\"Train accuracy: mean={np.mean(train_accs):.4f} ± {np.std(train_accs):.4f}\")\n",
    "print(f\"Valid accuracy: mean={np.mean(val_accs):.4f} ± {np.std(val_accs):.4f}\")\n",
    "\n",
    "\n",
    "# Fit final model on all data (optional sanity checks)\n",
    "final_model = LogisticRegression(solver=\"lbfgs\", max_iter=1000).fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb99087d-1fc5-40dc-9d50-3d972eb4bf4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np, torch, torch.nn as nn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# X: numeric DataFrame, y: {0,1} Series\n",
    "Xn, yn = X.values.astype(\"float32\"), y.values.astype(\"float32\").reshape(-1,1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "EPOCHS, FOLDS = 500, 5\n",
    "\n",
    "in_dim = Xn.shape[1]\n",
    "def make_model(h=64):\n",
    "    return nn.Sequential(nn.Linear(in_dim,h), nn.ReLU(), nn.Linear(h,1), nn.Sigmoid()).to(device)\n",
    "\n",
    "crit = nn.BCELoss()\n",
    "skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "tr_all, va_all = [], []\n",
    "tr_loss_final, va_loss_final = [], []\n",
    "tr_acc_final,  va_acc_final  = [], []\n",
    "\n",
    "for i, (tr_idx, va_idx) in enumerate(skf.split(Xn, yn.ravel()), 1):\n",
    "    Xtr, Xva = Xn[tr_idx], Xn[va_idx]; ytr, yva = yn[tr_idx], yn[va_idx]\n",
    "    sc = StandardScaler(); Xtr = sc.fit_transform(Xtr).astype(\"float32\"); Xva = sc.transform(Xva).astype(\"float32\")\n",
    "\n",
    "    Xtr_t = torch.from_numpy(Xtr).to(device); ytr_t = torch.from_numpy(ytr).to(device)\n",
    "    Xva_t = torch.from_numpy(Xva).to(device); yva_t = torch.from_numpy(yva).to(device)\n",
    "\n",
    "    m = make_model(); opt = torch.optim.Adam(m.parameters(), lr=1e-3)\n",
    "    tr_curve, va_curve = [], []\n",
    "\n",
    "    for _ in range(EPOCHS):\n",
    "        m.train(); opt.zero_grad()\n",
    "        p = m(Xtr_t); loss = crit(p, ytr_t); loss.backward(); opt.step()\n",
    "        tr_curve.append(loss.item())\n",
    "\n",
    "        m.eval()\n",
    "        with torch.no_grad():\n",
    "            pv = m(Xva_t); va_curve.append(crit(pv, yva_t).item())\n",
    "\n",
    "    # final epoch metrics\n",
    "    with torch.no_grad():\n",
    "        p  = m(Xtr_t); pv = m(Xva_t)\n",
    "        tr_acc = ( (p  >= 0.5).float().eq(ytr_t).float().mean().item() )\n",
    "        va_acc = ( (pv >= 0.5).float().eq(yva_t).float().mean().item() )\n",
    "\n",
    "    tr_all.append(tr_curve); va_all.append(va_curve)\n",
    "    tr_loss_final.append(tr_curve[-1]); va_loss_final.append(va_curve[-1])\n",
    "    tr_acc_final.append(tr_acc);       va_acc_final.append(va_acc)\n",
    "\n",
    "    print(f\"Fold {i}: train loss={tr_curve[-1]:.4f}, val loss={va_curve[-1]:.4f}, \"\n",
    "          f\"train acc={tr_acc:.4f}, val acc={va_acc:.4f}\")\n",
    "\n",
    "# ---- Averages across folds ----\n",
    "print(\"\\n=== Averages across folds ===\")\n",
    "print(f\"Train loss: {np.mean(tr_loss_final):.4f} ± {np.std(tr_loss_final):.4f}\")\n",
    "print(f\"Val   loss: {np.mean(va_loss_final):.4f} ± {np.std(va_loss_final):.4f}\")\n",
    "print(f\"Train acc : {np.mean(tr_acc_final):.4f} ± {np.std(tr_acc_final):.4f}\")\n",
    "print(f\"Val   acc : {np.mean(va_acc_final):.4f} ± {np.std(va_acc_final):.4f}\")\n",
    "\n",
    "# ---- Plot mean loss vs epoch ----\n",
    "tr_all, va_all = np.array(tr_all), np.array(va_all)\n",
    "epochs = np.arange(1, EPOCHS+1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, tr_all.mean(0), label=\"Train loss\")\n",
    "plt.plot(epochs, va_all.mean(0), label=\"Val loss\")\n",
    "plt.fill_between(epochs, tr_all.mean(0)-tr_all.std(0), tr_all.mean(0)+tr_all.std(0), alpha=0.2)\n",
    "plt.fill_between(epochs, va_all.mean(0)-va_all.std(0), va_all.mean(0)+va_all.std(0), alpha=0.2)\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Binary cross-entropy\")\n",
    "plt.title(f\"{FOLDS}-fold mean loss per epoch\"); plt.legend(); plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c50b6d16-952e-4978-a899-244cb8758610",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# assumes: trained PyTorch model `m` (ends with nn.Sigmoid),\n",
    "#          fitted scaler `sc`, and pandas DataFrame `X` (features)\n",
    "import numpy as np, torch, shap, matplotlib.pyplot as plt, torch.nn as nn\n",
    "\n",
    "m.eval()\n",
    "device = next(m.parameters()).device\n",
    "\n",
    "# copy model WITHOUT final Sigmoid for explaining logits (more stable)\n",
    "logit_model = nn.Sequential(*list(m.children())[:-1]).to(device).eval()\n",
    "\n",
    "# scale data (same scaler as training)\n",
    "Xs = sc.transform(X.values.astype(\"float32\"))\n",
    "rng = np.random.default_rng(42)\n",
    "bg_idx = rng.choice(len(Xs), size=min(200, len(Xs)), replace=False)\n",
    "ex_idx = rng.choice(len(Xs), size=min(1000, len(Xs)), replace=False)\n",
    "\n",
    "bg_t = torch.from_numpy(Xs[bg_idx]).to(device)\n",
    "ex_t = torch.from_numpy(Xs[ex_idx]).to(device)\n",
    "\n",
    "# --- SHAP DeepExplainer (disable additivity check at call time) ---\n",
    "explainer = shap.DeepExplainer(logit_model, bg_t)\n",
    "sv = explainer.shap_values(ex_t, check_additivity=False)  # <- key change\n",
    "\n",
    "# Normalize to (N, F) numpy\n",
    "if isinstance(sv, list): sv = sv[0]\n",
    "if torch.is_tensor(sv):  sv = sv.detach().cpu().numpy()\n",
    "sv = np.asarray(sv)\n",
    "sv = np.squeeze(sv)                     # handles (N,F,1)/(N,1,F) → (N,F)\n",
    "if sv.ndim == 1: sv = sv.reshape(-1, 1)\n",
    "\n",
    "# Global importance = mean |SHAP| per feature\n",
    "mean_abs = np.mean(np.abs(sv), axis=0)\n",
    "feat_names = list(X.columns)\n",
    "order = np.argsort(mean_abs)[::-1]\n",
    "\n",
    "plt.figure(figsize=(10, max(3, 0.4*len(feat_names))))\n",
    "plt.barh([feat_names[i] for i in order], mean_abs[order])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel(\"Mean |SHAP value| (logit units)\")\n",
    "plt.title(\"Feature importance (global)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a121f01-b34b-4c56-b3a0-c1b67ee5e449",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# df = your pandas DataFrame with columns \"cat__Sex_female\" and target \"survived\"/\"Survived\"\n",
    "x_col = \"cat__Sex_female\"\n",
    "y_col = 'num__Survived'\n",
    "\n",
    "x = X[x_col].astype(float).values\n",
    "y = pdf[y_col].astype(float).values\n",
    "\n",
    "# jitter so points don’t sit exactly at 0/1\n",
    "rng = np.random.default_rng(42)\n",
    "xj = x + rng.normal(0, 0.02, size=len(x))\n",
    "yj = y + rng.normal(0, 0.02, size=len(y))\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.scatter(xj, yj, s=10, alpha=0.35)\n",
    "plt.xticks([0, 1], [\"male (0)\", \"female (1)\"])\n",
    "plt.yticks([0, 1], [\"not survived (0)\", \"survived (1)\"])\n",
    "plt.xlabel(x_col)\n",
    "plt.ylabel(y_col)\n",
    "plt.title(\"cat__Sex_female vs survived (jittered scatter)\")\n",
    "\n",
    "# OPTIONAL: overlay mean survival per sex as big markers\n",
    "means = pdf.groupby(x_col)[y_col].mean()\n",
    "for xv, mv in means.items():\n",
    "    plt.scatter([xv], [mv], s=120, marker=\"X\")\n",
    "\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb43a46e-87c0-498d-b78d-5fd190c4d8e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# df = your pandas DataFrame with columns \"cat__Sex_female\" and target \"survived\"/\"Survived\"\n",
    "x_col = \"num__Pclass\"\n",
    "y_col = 'num__Survived'\n",
    "\n",
    "x = X[x_col].astype(float).values\n",
    "y = pdf[y_col].astype(float).values\n",
    "\n",
    "# jitter so points don’t sit exactly at 0/1\n",
    "rng = np.random.default_rng(42)\n",
    "xj = x + rng.normal(0, 0.02, size=len(x))\n",
    "yj = y + rng.normal(0, 0.02, size=len(y))\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.scatter(xj, yj, s=10, alpha=0.35)\n",
    "# plt.xticks([0, 1], [\"male (0)\", \"female (1)\"])\n",
    "plt.yticks([0, 1], [\"not survived (0)\", \"survived (1)\"])\n",
    "plt.xlabel(x_col)\n",
    "plt.ylabel(y_col)\n",
    "plt.title(\"cat__Sex_female vs survived (jittered scatter)\")\n",
    "\n",
    "# OPTIONAL: overlay mean survival per sex as big markers\n",
    "means = pdf.groupby(x_col)[y_col].mean()\n",
    "for xv, mv in means.items():\n",
    "    plt.scatter([xv], [mv], s=120, marker=\"X\")\n",
    "\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a9d5801-4724-4b3d-85df-552f987ebfba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from mlflow.models.signature import infer_signature\n",
    "import mlflow, mlflow.sklearn, mlflow.pytorch\n",
    "\n",
    "# --- set your UC location ---\n",
    "CATALOG = \"dp_ml_raw\"        # or your catalog, e.g. \"prod\", \"sandbox\"\n",
    "SCHEMA  = \"dp_ml_titanic\"     # or your schema, e.g. \"ml\", \"models\"\n",
    "\n",
    "spark.sql(f\"USE {CATALOG}.{SCHEMA}\")\n",
    "\n",
    "# --- prep ---\n",
    "X_np = X.values.astype(np.float32)\n",
    "y_np = pdf['num__Survived'].values.astype(int)\n",
    "scaler = StandardScaler().fit(X_np)                       # fit scaler\n",
    "\n",
    "# Logistic Regression (expects scaled input)\n",
    "logreg = LogisticRegression(max_iter=1000).fit(scaler.transform(X_np), y_np)\n",
    "\n",
    "# PyTorch model `m` already trained on scaled inputs:\n",
    "#   inputs: scaler.transform(X_np).astype(np.float32)\n",
    "#   outputs: probabilities in [0,1] (m ends with nn.Sigmoid)\n",
    "\n",
    "mlflow.set_experiment(\"/Shared/titanic-models\")\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    # -------- 1) StandardScaler --------\n",
    "    X_sig_df = X.head(50).astype(np.float64)\n",
    "    sig_scaler = infer_signature(\n",
    "        model_input=X_sig_df,\n",
    "        model_output=scaler.transform(X_sig_df.values).astype(np.float64)\n",
    "    )\n",
    "    info_scaler = mlflow.sklearn.log_model(\n",
    "        sk_model=scaler,\n",
    "        artifact_path=\"scaler\",\n",
    "        signature=sig_scaler,\n",
    "        input_example=X.head(5),\n",
    "        registered_model_name='titanic-scaler'\n",
    "    )\n",
    "\n",
    "    # -------- 2) Logistic Regression (expects scaled input) --------\n",
    "    Xs_sig = scaler.transform(X.head(50).values).astype(np.float64)\n",
    "    sig_logreg = infer_signature(\n",
    "        model_input=Xs_sig,\n",
    "        model_output=logreg.predict_proba(Xs_sig)\n",
    "    )\n",
    "    info_logreg = mlflow.sklearn.log_model(\n",
    "        sk_model=logreg,\n",
    "        artifact_path=\"logreg\",\n",
    "        signature=sig_logreg,\n",
    "        input_example=scaler.transform(X.head(5).values),\n",
    "        registered_model_name=\"titanic-logreg\"\n",
    "    )\n",
    "\n",
    "    # -------- 3) PyTorch NN (expects scaled input) --------\n",
    "    X_t_sig = scaler.transform(X.head(50).values).astype(np.float32)\n",
    "    # output example: probability column\n",
    "    y_t_sig = np.zeros((X_t_sig.shape[0], 1), dtype=np.float32)\n",
    "    sig_torch = infer_signature(model_input=X_t_sig, model_output=y_t_sig)\n",
    "\n",
    "    info_torch = mlflow.pytorch.log_model(\n",
    "        pytorch_model=m,\n",
    "        artifact_path=\"torch_nn\",\n",
    "        signature=sig_torch,\n",
    "        input_example=X_t_sig[:5],\n",
    "        registered_model_name=\"titanic-pytorch\"\n",
    "    )\n",
    "    mlflow.log_metric(\"val_accuracy_logreg\", float(f\"{np.mean(val_accs):.4f}\"))\n",
    "    mlflow.log_metric(\"val_accuracy_torch_nn\", float(f\"{np.mean(va_acc_final):.4f}\"))\n",
    "\n",
    "print(\"Logged URIs:\")\n",
    "print(\"  Scaler  :\", info_scaler.model_uri)\n",
    "print(\"  LogReg  :\", info_logreg.model_uri)\n",
    "print(\"  Torch NN:\", info_torch.model_uri)\n",
    "\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# set alias for that new version\n",
    "client = MlflowClient()\n",
    "\n",
    "\n",
    "\n",
    "LOGREG_NAME_UC = f\"{CATALOG}.{SCHEMA}.titanic-logreg\"\n",
    "TORCH_NAME_UC  = f\"{CATALOG}.{SCHEMA}.titanic-pytorch\"\n",
    "\n",
    "if np.mean(val_accs) > np.mean(va_acc_final): \n",
    "    reg_name = LOGREG_NAME_UC\n",
    "    run_id = info_logreg.run_id\n",
    "else: \n",
    "    reg_name = TORCH_NAME_UC\n",
    "    run_id = info_torch.run_id\n",
    "\n",
    "# find the model version created in this run\n",
    "mvs = client.search_model_versions(f\"name='{reg_name}'\")\n",
    "this_mv = [mv for mv in mvs if mv.run_id == run_id][0]  # the version from *this* run\n",
    "version = int(this_mv.version)\n",
    "\n",
    "# set the alias\n",
    "client.set_registered_model_alias(\n",
    "    name=reg_name,\n",
    "    alias=\"champion\",\n",
    "    version=version\n",
    ")\n",
    "\n",
    "print(f\"Alias 'champion' now points to {reg_name} v{version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "829338b9-cff3-472f-bbea-3d6546311c28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "⠀⠀⠀⠀⠀⠀⢹⣄⣿⣦⣼⣆⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⢨⣿⣿⣿⣿⠿⣷⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⢸⣿⠟⠋⠀⠀⠘⣧⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⢠⣟⣁⣴⡄⠀⠀⠀⠘⣧⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⢿⠛⢉⣠⠀⠀⠀⠀⠸⣧⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠈⣿⣿⠏⠀⠀⠀⠀⠀⠸⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠸⡇⠀⠀⠀⠀⠀⠀⠀⢹⣆⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⣧⣤⣶⠖⠀⠀⠀⠀⠀⢻⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⢻⡉⠁⠀⣀⣤⠀⠀⠀⠀⢷⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠈⣿⠛⠛⠛⠁⠀⠀⠀⠀⠘⣧⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⢹⣦⣤⣶⠿⠀⣀⣤⣤⣶⣿⣿⠒⢤⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢿⠁⢀⣴⣿⣿⣿⣿⣿⣿⣿⡄⣤⣾⣷⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⣷⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡟⣹⣿⣿⣿⣿⣿⣿⠿⣿⣿⣿⣿⣿⣿⣿⣷⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣧⣿⣿⣿⣿⣿⡿⠃⠀⠀⠻⣿⣿⣿⣿⣿⣿⠛⢦⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢹⣿⠿⠿⣿⠟⠁⠀⠀⠀⠀⠈⠻⢿⣿⣿⡇⢰⣿⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢻⡀⠀⠀⠀⠀⠀⠀⠀⢀⠀⠀⠀⠉⠉⠁⢞⣿⠈⣦⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣳⡄⠀⠀⠀⣠⣴⣾⣯⠖⠀⠀⠀⠀⠀⠹⡇⣰⣿⢧⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡴⠛⠉⢻⣿⣶⣆⠈⠉⠉⠀⠀⠀⠀⠀⠀⠀⠀⢧⢉⠇⠈⢷⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⢀⡴⠋⠀⢀⡴⠟⢻⡉⢿⠀⠀⣠⡤⠀⢀⡀⠀⠀⠀⠀⣾⡞⠀⠀⠀⠈⠳⣄⡀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⢀⡴⣋⣤⣀⣰⠏⠀⠀⢸⠳⣼⣦⣴⣿⣿⣿⡿⠛⠀⠀⢀⣼⠟⠀⠀⠀⠀⠀⠀⠈⠙⢦⡀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⢠⣏⣴⣿⣿⣿⣿⠀⠀⢀⣿⣶⣿⣿⣿⡋⠭⠀⠀⠀⠀⣠⠟⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⢦⠀⠀⠀⠀\n",
    "⠀⠀⠀⢀⣴⠿⠛⠛⠛⢿⣿⣿⣶⣾⣿⣿⣿⣿⣿⣿⠿⣶⣶⣴⣶⠟⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⣧⠀⠀⠀\n",
    "⠀⠀⠠⣿⣁⣀⡀⠀⢀⣠⣽⣿⠿⠿⠿⣿⣿⣿⣿⠏⠀⠀⠉⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠹⡄⠀⠀\n",
    "⠀⠀⠀⠈⠉⣻⠇⠀⠘⠋⠁⣀⣀⣤⣴⠾⠟⣿⡟⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢷⠀⠀\n",
    "⠀⠀⢀⣴⠟⠋⠀⢠⣶⡾⠿⠛⣛⣩⣤⣤⣤⡿⠀⠀⠀⠀⠀⠀⠀⢀⣰⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣰⠀⠘⡆⠀\n",
    "⠀⠀⠸⣧⡀⠀⣀⠀⠀⣠⣶⠟⠛⠉⠉⠉⣽⠃⠀⠀⠀⠀⢀⣤⠾⠛⢉⡇⠀⠀⢀⣤⣶⠖⠀⠀⠀⠀⠀⠀⢸⠀⠀⢹⡀\n",
    "⠀⠀⠀⠈⠛⠛⢻⡆⠀⠈⠀⣠⣶⠶⠿⢾⡟⠀⠀⠀⣠⡾⠟⠁⣠⠖⠋⣷⣠⣾⠟⠋⠁⠀⠀⠀⠀⠀⠀⠀⠈⣇⠀⠀⣇\n",
    "⠀⠀⠀⠀⠀⣠⣼⠇⠀⠀⠀⠉⠀⢀⣀⣿⠃⠀⢠⠚⠉⢀⡴⠟⠁⠀⢀⣸⣿⣀⣤⣶⣶⣦⠀⠀⠀⠀⠀⠀⠀⢹⡀⠀⡿\n",
    "⠀⠀⠀⠀⣾⠋⠀⠀⠀⠀⠀⠀⠐⠛⢻⡟⠀⠀⢸⣠⡶⠋⠀⠀⣠⡶⠟⠉⣿⡟⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⣧⡼⠁\n",
    "⠀⠀⠀⠀⠻⣦⣤⣤⣄⠀⠀⠀⠀⠀⣿⠃⠀⠀⠈⠉⠀⣀⡴⠟⠉⠀⢀⣠⣿⣷⢀⣤⣴⡶⠄⠀⠀⠀⠀⠀⠀⠀⢿⠁⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠈⣿⡀⢀⣀⣴⣾⡏⠀⠀⠀⢀⣤⣾⠋⠀⠀⣀⡴⠛⠁⠀⢻⡟⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⡇⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⣀⣿⠿⠛⠉⢸⣿⠇⠀⠀⠀⠈⡏⢿⣀⣴⠞⠉⠀⠀⣀⡴⠚⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⡇⠀\n",
    "⠀⠀⠀⢀⣤⠶⠛⠉⠀⠀⠀⠀⣼⡿⠀⠀⠀⠀⠀⠙⠼⠋⠁⠀⢀⣤⠞⠉⠀⠀⢻⣤⡾⠷⠆⠀⠀⠀⠀⠀⠀⠀⠀⣷⠀\n",
    "⠀⣠⡾⠋⠁⠀⣠⣴⠿⠛⣻⡿⢿⡇⠀⠀⠀⠀⠀⠀⠀⣠⣴⡞⠋⠀⠀⠀⢀⡤⠿⣧⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⠀\n",
    "⣰⠏⠀⠀⠀⠘⣋⣡⠶⠛⠁⠀⢸⡇⠀⠀⠀⠀⠀⢠⢺⠋⢻⡷⣀⣀⣤⠞⠋⠀⠀⣿⣆⣀⣀⣀⡀⠀⠀⠀⠀⠀⠀⣿⠀\n",
    "⠙⠷⠶⠶⠶⠛⠉⠀⠀⠀⠀⠀⢸⠇⠀⠀⠀⠀⠀⠀⢣⣣⣀⠳⡽⠋⠀⠀⠀⢀⣴⢿⣿⠛⠋⠉⠀⠀⠀⠀⠀⠀⠀⣿⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣾⠀⠀⠀⠀⠀⠀⠀⠀⠙⠓⠋⠀⠀⣀⣠⡶⠋⠀⣼⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⡏⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣰⢫⣿⢿⡀⠀⣠⣿⡏⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠇⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠛⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⠘⠊⠙⠙⠚⠛⠛⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠛⠀⠀"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "train",
   "widgets": {
    "features_table": {
     "currentValue": "titanic_features",
     "nuid": "118a7ef6-1158-4267-b857-98ba0fe7ab15",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "titanic_features",
      "label": null,
      "name": "features_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "titanic_features",
      "label": null,
      "name": "features_table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
